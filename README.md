# rag-doc-bot
A **documentâ€‘grounded Retrievalâ€‘Augmented Generation (RAG) chatbot** built with FastAPI, ChromaDB, and a local AI model via Ollama. Streamlined React chat UI with chat session and document management.

---

## ðŸŒŸ Features

| Feature                    | Behavior                                                             |
|----------------------------|----------------------------------------------------------------------|
| **Chat Session management**     | Create, name, view, and delete chat sessions with title autoâ€‘generated from first user message |
| **File support**           | Upload PDF, TXT, DOCX to link documents to specific sessions         |
| **Text extraction**        | Converts files into chunks ready for embedding generation            |
| **Semantic indexing**      | Embeddings created with Sentenceâ€‘Transformers and stored in ChromaDB  [oai_citation:0â€¡FreeCodeCamp](https://www.freecodecamp.org/news/how-to-run-open-source-llms-locally-using-ollama/?utm_source=chatgpt.com) [oai_citation:1â€¡GitHub](https://github.com/langchain-ai/langchain/discussions/7818?utm_source=chatgpt.com) [oai_citation:2â€¡DEV Community](https://dev.to/msbala007/building-a-simple-powerful-chatbot-using-chromadb-and-sentence-transformers-4md?utm_source=chatgpt.com) |
| **Proactive retrieval**    | Topâ€‘K chunk search per query, optimized for speed and relevance     |
| **Streaming replies**      | Assistant messages streamed chunkâ€‘wise for fluid UX                  |
| **Webâ€‘based chat UI**      | React chat interface with session sidebar, editable titles, and smooth scrolling |
| **Local LLM engine**       | Uses Ollama for local, offline inference with private data support   [oai_citation:3â€¡FreeCodeCamp](https://www.freecodecamp.org/news/how-to-run-open-source-llms-locally-using-ollama/?utm_source=chatgpt.com) [oai_citation:4â€¡LangChain](https://python.langchain.com/docs/how_to/local_llms/?utm_source=chatgpt.com) |
| **CI/CD ready**            | GitHub Actions workflow with backâ€‘end pytest and frontâ€‘end Vitest tests |
| **Dockerâ€‘compose deployment** | Full stack up via `docker-compose up --build`                      |

---

## ðŸš€ Value Proposition

Built for knowledge teams, recruiters, and power users to quickly index documents and engage with them using chat. With enterpriseâ€‘grade features like secure local LLM inference and sessionâ€‘bound contexts, RAGâ€‘Docâ€‘Bot allows:

- Instant insights from uploaded documents  
- Customizable conversation flow per session  
- Faster knowledge retrieval than manual search  
- Full auditability and control over chat data and embeddings

---

## ðŸ§± Tech Stack

- **FastAPI** â€“ core backâ€‘end REST API and ingest/travel logic  
- **SQLModel** â€“ PostgreSQL backed models for session & message storage  
- **ChromaDB** â€“ local vector database optimized for semantic search  [oai_citation:5â€¡docs.datarobot.com](https://docs.datarobot.com/en/docs/gen-ai/genai-code/chromadb-vdb.html?utm_source=chatgpt.com) [oai_citation:6â€¡DEV Community](https://dev.to/msbala007/building-a-simple-powerful-chatbot-using-chromadb-and-sentence-transformers-4md?utm_source=chatgpt.com) [oai_citation:7â€¡Analytics Vidhya](https://www.analyticsvidhya.com/blog/2023/07/guide-to-chroma-db-a-vector-store-for-your-generative-ai-llms/?utm_source=chatgpt.com)  
- **Sentenceâ€‘Transformers** â€“ embedding model (e.g. `allâ€‘MiniLMâ€‘L6â€‘v2`)  
- **Ollama** â€“ runs LLM locally; full privacy and offline inference  [oai_citation:8â€¡Hugging Face](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2?utm_source=chatgpt.com) [oai_citation:9â€¡FreeCodeCamp](https://www.freecodecamp.org/news/how-to-run-open-source-llms-locally-using-ollama/?utm_source=chatgpt.com)  
- **React (TypeScript)** â€“ intuitive chat UI, live messaging, dragâ€‘andâ€‘drop uploads  
- **TanStack Query** â€“ data fetching & state management  
- **Tailwind CSS** â€“ responsive styling and animation  


## ðŸ›  Installation & Usage

### Prerequisites

- Python 3.11+, Postgres (if not using Docker)
- Node.js v18+ (or 20.x)
- [Ollama](https://ollama.com/) installed and a local LLM downloaded (e.g., `ollama pull llama3.2`)

### Quick Start (Docker)

```bash
cd /path/to/project
docker-compose up --build
```
### Manual startup (dev mode)
```bash
# Backend
python3 -m venv venv
source venv/bin/activate
pip install -r backend/requirements.txt
uvicorn backend.fastapi.main:app --reload

# Frontend
cd frontend
npm install
npm run dev
```

### Running Tests
```bash
pytest backend/tests --junitxml=backend-test-reports/junit.xml

# Frontend
cd frontend
npm install
CI=true npx vitest run --coverage
```
