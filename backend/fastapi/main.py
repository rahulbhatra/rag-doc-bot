from backend.models.message import Message
from typing import List, Optional
from fastapi import Depends, FastAPI
from pydantic import BaseModel
from backend.database.psql import get_session, init_db
from backend.ingestion.retrival import retrieve_top_k
from backend.llm.llm import ask_ollama
from backend.fastapi.session import router as session_router
from backend.fastapi.document import router as document_router
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
import json
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

app = FastAPI(on_startup=[init_db])

# CORS setup if frontend on a different port
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class QueryRequest(BaseModel):
    question: str
    session_id: int
    top_k: int = 3

class QueryResponse(BaseModel):
    answer: str
    chunks: Optional[List[dict]] = None  # includes text, metadata, distance

async def get_recent_messages(session: AsyncSession, session_id: int, limit: int = 100):
    result = await session.execute(
        select(Message).where(Message.session_id == session_id).order_by(Message.timestamp.desc()).limit(limit)
    )
    messages = result.scalars().all()
    return list(reversed(messages))

@app.post("/query", response_model=QueryResponse)
async def query_endpoint(req: QueryRequest, db: AsyncSession = Depends(get_session)):
    question = req.question
    session_id = req.session_id
    top_k = req.top_k

    # Retrieve chat history
    recent_msgs = await get_recent_messages(db, session_id)

    # Convert history to string
    chat_history = "\n".join([f"{msg.role.capitalize()}: {msg.text}" for msg in recent_msgs])

    # Step 1: Retrieve relevant context chunks
    docs = retrieve_top_k(question, session_id, top_k)
    context = "\n".join([doc[0] for doc in docs])
    print(f"Context generated by retrieve top k, {context}, {docs}")

    # Step 2: Build prompt
    prompt = f"Context:\n{context}\n\nQuestion: {question}\nAnswer:"

    # Step 3: Ask Ollama
    def stream():
        for chunk in ask_ollama(prompt):  # This must be a generator
            yield f"data: {json.dumps(chunk)}\n\n"

    return StreamingResponse(stream(), media_type="text/event-stream")

app.include_router(session_router)
app.include_router(document_router)